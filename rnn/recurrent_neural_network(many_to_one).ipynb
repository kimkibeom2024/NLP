{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02026a2-735b-4f48-873b-e182ed402eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f37a5c8d6f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a0145f-0370-4c30-a6f6-26e648dc4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, word_dict, window_size=3):\n",
    "        self.sentences = sentences\n",
    "        self.word_dict = word_dict\n",
    "        self.window_size = window_size\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        \n",
    "        # 데이터 전처리 : 짧은 시퀀스 - 패딩 추가, 긴 시퀀스 - 삭제ss CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, word_dict, window_size=3):\n",
    "        self.sentences = sentences\n",
    "        self.word_dict = word_dict\n",
    "        self.window_size = window_size\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        \n",
    "        # 데이터 전처리 : 짧은 시퀀스 - 패딩 추가, 긴 시퀀스 - 삭제\n",
    "        self.X, self.y = self.make_batch()\n",
    "\n",
    "    def make_batch(self):\n",
    "        _X = []\n",
    "        _y = []\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split()\n",
    "            \n",
    "            # Add padding if the sentence is shorter than window_size\n",
    "            if len(words) < self.window_size:\n",
    "                words = [self.PAD_TOKEN] * (self.window_size - len(words)) + words\n",
    "            \n",
    "            # Convert words to indices and handle padding\n",
    "            X = [self.word_dict.get(word, self.word_dict[self.PAD_TOKEN]) for word in words[:self.window_size]]\n",
    "            y = self.word_dict[words[self.window_size]] if len(words) > self.window_size else self.word_dict[self.PAD_TOKEN]\n",
    "\n",
    "            _X.append(X)\n",
    "            _y.append(y)\n",
    "        \n",
    "        return _X, _y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b131cb42-4e43-44e4-95e6-dc208bd806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model definition with embedding layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        # Embedding layer for converting words to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, 128)\n",
    "        self.rnn = nn.RNN(input_size=128, hidden_size=hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        # Convert word indices to embedding vectors\n",
    "        X = self.embedding(X)\n",
    "        \n",
    "        # Transpose batch and sequence dimensions\n",
    "        X = X.transpose(0, 1)\n",
    "        \n",
    "        hidden_vectors, hidden = self.rnn(X, hidden)\n",
    "        last_hidden_vector = hidden_vectors[-1]\n",
    "        \n",
    "        output = self.fc(last_hidden_vector)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95647513-058e-4ec7-95d5-eb2121ed5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for RNN\n",
    "def train(model, dataloader, epochs, vocab_size):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            hidden = model.init_hidden(batch_X.size(0))\n",
    "            output = model(hidden, batch_X)\n",
    "            \n",
    "            loss = criterion(output, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:04d}, Loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbf8bd5-693a-4d26-955e-efc5b00e3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function for short sentences\n",
    "def predict(model, input_sentence, word_dict, number_dict, window_size=3):\n",
    "    model.eval()\n",
    "    \n",
    "    words = input_sentence.split()\n",
    "    \n",
    "    input_indices = [word_dict.get(w, word_dict[\"<PAD>\"]) for w in words]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long)\n",
    "    \n",
    "    # RNN의 초기 hidden state 설정 (배치 크기는 1)\n",
    "    hidden = model.init_hidden(1)\n",
    "    output = model(hidden, input_tensor)\n",
    "    \n",
    "    # 예측된 단어의 인덱스 추출\n",
    "    predicted_idx = output.data.max(1, keepdim=True)[1]\n",
    "    predicted_word = number_dict[predicted_idx.item()]\n",
    "    \n",
    "    return f\"Input Sentence: {input_sentence} -> Predicted Word: {predicted_word}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20a0ea8-d9da-47aa-8eb3-5d698558a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset from file\n",
    "def load_dataset(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences.append(line.strip())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bd57ba-831f-48e8-b556-017808127b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length : 50\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and prepare word dictionaries\n",
    "sentences = load_dataset('./dataset.txt')\n",
    "\n",
    "# Build vocabulary\n",
    "texts = list(set(\" \".join(sentences).split())) + [\"<PAD>\"]\n",
    "texts.sort()\n",
    "\n",
    "word_dict = {text: index for index, text in enumerate(texts)}\n",
    "number_dict = {index: text for index, text in enumerate(texts)}\n",
    "vocab_size = len(word_dict)\n",
    "hidden_size = 32\n",
    "window_size = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the dataset and DataLoader\n",
    "dataset = CustomDataset(sentences, word_dict, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f'Dataset length : {dataset.__len__()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62bb601a-1e6b-45a0-8cca-094ebaea1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [00:00<00:04, 218.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, Loss: 4.433485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [00:00<00:04, 175.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100, Loss: 0.001807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 229/1000 [00:01<00:04, 177.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0200, Loss: 0.000813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 323/1000 [00:01<00:03, 186.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0300, Loss: 0.000304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 427/1000 [00:02<00:02, 207.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400, Loss: 0.000313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 531/1000 [00:02<00:01, 246.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0500, Loss: 0.000168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 635/1000 [00:03<00:01, 253.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0600, Loss: 0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 745/1000 [00:03<00:00, 266.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0700, Loss: 0.000108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 834/1000 [00:03<00:00, 284.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0800, Loss: 0.000086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 956/1000 [00:04<00:00, 290.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0900, Loss: 0.000085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 227.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "model = RNN(vocab_size, hidden_size)\n",
    "train(model, dataloader, epochs=1000, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "052bcbfc-eb53-4553-b185-e01fcbd1da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I like to -> Predicted Word: sing\n"
     ]
    }
   ],
   "source": [
    "# Predict for a given sentence\n",
    "input_sentence = \"I like to\"\n",
    "print(predict(model, input_sentence, word_dict, number_dict, window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195b694-a524-4609-9aaf-839ce0cb0834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kibeom)",
   "language": "python",
   "name": "langchain-kr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
